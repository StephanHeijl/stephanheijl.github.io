<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Notes on BERT - Stephan Heijl</title>
    <meta name="description" content="Detailed notes on Google's BERT paper: Pre-training of Deep Bidirectional Transformers for Language Understanding.">
    <link href="https://fonts.googleapis.com/css2?family=Newsreader:ital,opsz,wght@0,6..72,400;0,6..72,500;1,6..72,400&family=Outfit:wght@300;400;500;600&family=JetBrains+Mono:wght@400&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="style-v2.css">
    <style>
        .article-content table {
            width: 100%;
            border-collapse: collapse;
            margin: 2rem 0;
            font-size: 0.9rem;
        }
        .article-content thead {
            background: var(--bg-secondary);
            font-weight: 600;
        }
        .article-content th, .article-content td {
            padding: 0.75rem 1rem;
            text-align: left;
            border-bottom: 1px solid var(--border-subtle);
        }
        .article-content tr:hover {
            background: var(--bg-secondary);
        }
        .section-header {
            background: var(--bg-secondary);
            padding: 0.5rem 1rem;
            font-style: italic;
        }
    </style>
</head>
<body>
    <!-- Progress Bar -->
    <div class="progress-bar" id="progressBar"></div>

    <!-- Navigation -->
    <nav>
        <a href="index.html" class="logo">Stephan Heijl</a>
        <ul class="nav-links">
            <li><a href="projects.html">Work</a></li>
            <li><a href="projects.html#posts">Writing</a></li>
            <li><a href="projects.html#education">Background</a></li>
            <li><a href="contact.html">Contact</a></li>
        </ul>
    </nav>

    <!-- Article Header -->
    <header class="article-header">
        <div class="breadcrumb">
            <a href="index.html">Home</a>
            <span>-></span>
            <a href="projects.html#posts">Writing</a>
            <span>-></span>
            <span>Notes on BERT</span>
        </div>

        <div class="article-meta">
            <span class="meta-tag">Paper Notes</span>
            <span class="meta-item">Posted: July 7, 2019</span>
            <span class="meta-item">Updated: August 4, 2020</span>
        </div>

        <h1>Notes on BERT: Pre-training Deep Bidirectional Transformers</h1>

        <p class="article-intro">
            As part of a self-imposed challenge to write more, I read machine learning papers and write notes
            on what stands out. This examines Google's BERT NLP <a href="https://arxiv.org/pdf/1810.04805.pdf">paper</a>.
        </p>
    </header>

    <!-- Article Content -->
    <article class="article-content">
        <h2>Technical Notes</h2>

        <table>
            <thead>
                <tr>
                    <th>Setting</th>
                    <th>Pretraining</th>
                    <th>Finetuning</th>
                </tr>
            </thead>
            <tbody>
                <tr class="section-header">
                    <td colspan="3">Adam settings</td>
                </tr>
                <tr>
                    <td>Learning rate</td>
                    <td>1e-4</td>
                    <td>Best of [5e-5, 4e-5, 3e-5, 2e-5]</td>
                </tr>
                <tr>
                    <td>Learning rate warmup</td>
                    <td>10000 steps</td>
                    <td>10000 steps</td>
                </tr>
                <tr>
                    <td>L2 weight decay</td>
                    <td>0.01</td>
                    <td>0.01</td>
                </tr>
                <tr>
                    <td>β1 / β2</td>
                    <td>0.9 / 0.99</td>
                    <td>0.9 / 0.99</td>
                </tr>
                <tr class="section-header">
                    <td colspan="3">Architecture settings</td>
                </tr>
                <tr>
                    <td>Dropout</td>
                    <td>0.1</td>
                    <td>0.1</td>
                </tr>
                <tr>
                    <td>Activation</td>
                    <td>GELU</td>
                    <td>GELU</td>
                </tr>
                <tr class="section-header">
                    <td colspan="3">Training settings</td>
                </tr>
                <tr>
                    <td>Batch size</td>
                    <td>256</td>
                    <td>Best of [16, 32]</td>
                </tr>
                <tr>
                    <td>Epochs</td>
                    <td>40</td>
                    <td>Best of [2, 3, 4]</td>
                </tr>
            </tbody>
        </table>

        <h2>Summary</h2>

        <p>
            On the whole, this paper was easy to read, with very little mathematics included. The focus was
            primarily on empirical results; technical decisions were offloaded to the Transformers paper
            (Attention is all you need).
        </p>

        <p>
            The BERT paper introduces a neural network based on the Transformer architecture which serves as
            a base for myriad NLP tasks. The model is pretrained on a very large corpus (3.3 billion words)
            and users need only make small refinements to adjust to specific tasks.
        </p>

        <h3>Pretraining Tasks</h3>

        <p>
            <strong>Masked Language Model:</strong> The model is fed sentences with masked tokens and asked
            to produce the true vocabulary ID of the missing word(piece). I found this technique similar to
            skip-grams in Word2Vec. Some strategies, like replacing [MASK] with random words, adjust the
            model to real-world tasks.
        </p>

        <p>
            <strong>Sentence Prediction:</strong> BERT is presented with two sentences. Sometimes consecutive,
            sometimes a non-sequitur randomly picked from the dataset. This improves the dual segment component.
        </p>

        <h3>Key Observations</h3>

        <ul>
            <li>Inputs are represented by token embedding + segment embedding + position embedding</li>
            <li>For classification, activations for the [CLS] token represent the entire input</li>
            <li>First 10,000 steps use learning rate warmup (linear increase from 0 to target)</li>
            <li>Fine-tuning requires only 3-4 epochs with relatively low learning rates</li>
        </ul>

        <h3>Final Notes</h3>

        <ul>
            <li>Appreciated the ablation experiment showing influence of different pretraining tasks</li>
            <li>Adding BiLSTM on top of left-to-right pretraining improved SQUAD but decreased GLUE performance</li>
            <li>Many tasks have ELMo baselines for useful comparison</li>
            <li>SQUAD & GLUE incorporate rigorous testing platforms for future algorithm evaluation</li>
        </ul>
    </article>

    <!-- Article Footer -->
    <div class="article-footer">
        <a href="projects.html#posts" class="back-link">
            <svg width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1.5">
                <path d="M19 12H5M12 19l-7-7 7-7"/>
            </svg>
            Back to Writing
        </a>
    </div>

    <!-- Footer -->
    <footer>
        <p class="footer-text">© 2024 Stephan Heijl</p>
        <div class="footer-links">
            <a href="https://github.com/StephanHeijl">GitHub</a>
            <a href="https://linkedin.com/in/stephanheijl">LinkedIn</a>
            <a href="contact.html">Email</a>
        </div>
    </footer>

    <script>
        window.addEventListener('scroll', () => {
            const docHeight = document.documentElement.scrollHeight - window.innerHeight;
            const progress = (window.scrollY / docHeight) * 100;
            document.getElementById('progressBar').style.width = progress + '%';
        });
    </script>
</body>
</html>
