<html>
    <head>
        <title>StephanHeijl.com</title>
        <link rel="stylesheet" href="style.css" />
        <link rel="stylesheet" href="gradients.css" />
        <script src="zepto.min.js"></script>
        <script src="cards.js"></script>
        <link href="https://fonts.googleapis.com/css?family=Dosis|Open+Sans" rel="stylesheet">

        <script src="https://use.fontawesome.com/73f3b30236.js"></script>
        <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" />
    </head>
    <body>
        <nav>
            <div class="internal-wrapper">
                <a class="logo" href="https://stephanheijl.com">StephanHeijl.com</a>
                <ul>
                    <li><a href="projects.html#projects">Projects</a></li>
                    <!--<li><a href="projects.html#publications">Publications</a></li>-->
                    <li><a href="projects.html#posts">Blog posts</a></li>
                    <li><a href="projects.html#education">Education</a></li>
                    <li><a href="contact.html">Contact</a></li>
                </ul>            
            </div>                     
        </nav>
        <header>
            <div class="internal-wrapper">
                <div class="header-card">
                    <div class="card-wrapper">
                        <div class="card gradient-paper">
                            <div class="glare"></div>
                            <div class="post-name">
                                Sequence Data in ML <br>
                                <i class="fa fa-link"></i>
                            </div>
                        </div>
                    </div>
                </div>
                <div class="header-annotation">
                    <h1>Python Tutorial: Using protein sequences to make better classifiers</h1>
                    <p>
                        Intro
                    </p>
                    <ul class="header-tags">
                        <li>Education</li>
                    </ul>
                </div>
            </div>
        </header>
        
        <div class="details">
            <div class="internal-wrapper">
                <h2>Introduction</h2>
                
                <p>
                    In bioinformatics, we often encounter situations where empirical data on some biological phenomenon is available and we would like to predict whether this also occurs in other cases. Many times, we find that interactions between variables are complex and not directly correlated to what is being studied. In those cases we can use machine learning to make these predictions for us. Machine learning has led to many revolutionary developments in the biological world [cite samples here]. What if, instead of just using straightforward tabular data or image data, we could also use protein sequence data in solving these problems?
                </p>
                
                <p>If you're already comfortable with the advantages of using protein sequences and the theory behind it, click <a href="#solution">here</a>. If not, read on!</p>
                
                <h3>Use cases</h3>
                <p>List some use cases here</p>

                
                <h3>Problem description</h3>
                <p>There are many different algorithms that can be used to perform machine learning, from the simplest linear models to complex neural networks. Most of these methods, however, have difficulty dealing with sequence data, for the following reasons:</p>
                
                <ul class="chevron-list">
                    <li>Sequences can vary in length.</li>
                    <li>In sequences, relative position matters, absolute position not so much.</li>
                    <li>Sequences are symbolic, not real-valued.</li>
                    <li>Sequences are not amenable to resizing in the way that images are.</li>
                    <li>Biological data is often sparse, which makes overfitting to highly dimensional data likely.</li>
                </ul>
                
                <p>A naive way in which one would integrate protein sequences into, for example, a Random Forest, would be to pad the sequence to the length of the longest protein, convert the symbols to a list of one-hot-vectors and creating a very large sparse matrix that way. This immediately leads to a set of problems:</p>
                
                <ul class="chevron-list">
                    <li>You are automatically limited to the length of the longest sequence in your training set.</li>
                    <li>Even when working with short sequences (&lt; 250 amino acids) this method will quickly result in 5250 features per sample. (250 * 21, as there are 20 different amino acids and one padding value), way too many variables for smaller datasets.</li>
                    <li>If a sequence is observed with a simple insertion in the middle of the sequence, it will throw off the algorithm, as feature influence is dependent on absolute position, not relative position.</li>
                </ul>               
                
                <p>This approach will thus lead to overfitting at best, or a lack of fitting at all in the worst case scenario. A better way is needed to incorporate sequence data into machine learning algorithms.</p>
                
                <h2>Theory</h2>
                
                <h3>Sequences</h3>
                
                <p>Sequences can generally be defined as a collection of objects in a certain order where repetition is allowed. They occur in many different shapes and sizes. A single number can be regarded as a sequence containing one number, and the entire works of Shakespeare can be seen as a long sequence of words (or characters). In Python sequences are often represented in strings, lists or numpy arrays. Sets are generally not used, as repetition is excluded by definition.</p>
                
                <p>Protein sequences are sequences of symbols, generally 20 different characters representing the 20 used amino acids used in human proteins. Proteins sequences can range from the very short (20 amino acids in total, https://www.science20.com/princerain/blog/smallest_protein) to the very long (38 183 amino acids for Titin, https://www.ncbi.nlm.nih.gov/pubmed/11717165). The average human protein comes in at around 375 amino acids (http://book.bionumbers.org/how-big-is-the-average-protein/). </p>
                
                <p>There are several ways in which sequences can be represented for use in machine learning purposes.</p>
                
                <ul class="chevron-list">
                    <li>Padded, with the different symbols as categoricals. This is the easiest representation to deliver, but it is rarely supported natively by more basic algorithms. More advanced algorithms like CatBoost do have support for categorical features.</li>
                    <li>Padded, but with the symbols replaced by some integer. While easy to generate, this approach is not very effective in a machine learning context, as the relationship between different amino acids is not linear and cannot be described in a single dimension. </li>
                    <li>Padded and converted into one-hot vectors. This yields a matrix which can be consumed by most machine learning algorithms, but due to 20+ times increase in feature count this approach quickly becomes untenable for proteins longer than a few residues.</li>
                    <li>An alternative, real valued representation with a fixed dimensionality, describing the sequence in its entirety.</li>
                </ul>
                
                <p>The final option is what will be discussed in this article. There are ways to generate such a description using heuristics and statistics on the sequence (GC content, total hydrophobicity, mean charge, etcetera), but here we will look at machine generated representations, also known as embeddings.</p>
                
                <h3>Embeddings</h3>
                
                <p>Neural networks have always had to contend with the issue of representation. Given that they consist of a pile of linear algebra (https://xkcd.com/1838/) only numbers can be inserted into the mix, with categorical data generally being reduced to a one-hot encoding in order for the network to accept it. However, this approach generally does not yield great results when the vocabulary is very large, as the number of features quickly gets out of hand. This was the case in natural language processing. Hence, neural networks were first used in 2003 by Bengio et al. (https://link.springer.com/chapter/10.1007%2F3-540-33486-6_6) to produce distributed representations of words. </p>
                
                <p>This concept was later expanded upon by a team of researchers at Google, who produced Word2Vec (https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality), a fast generator of word vectors, which are real valued representations of words that describe their meaning in N-dimensional space. One can even perform algebraic operations on these representations to derive the meaning from different words. This is exemplified by the now-famous “king - man = queen” expression.</p>
                
                <p>When working with sentences or even paragraphs, one can use any of a number of techniques to reduce a sequence of these vectors to a fixed number of dimensions. These include averaging, summing or performing more advanced dimensionality reduction techniques. We will get into those later in this post.</p>
                
                <h3>Transfer learning</h3>
                <p>Another problem that is often encountered in biological datasets, is their size. While the data may be detailed, there may only be a few thousand samples, as data gathering is an expensive endeavor. Any representation of a sequence tends to be high-dimensional, which can result in overfitting to the training set if proper precautions are not taken. It therefore makes sense not to start learning from the sequence directly, but instead find a representation that already has meaning attached to it. This approach imbues your model with information from outside your dataset, even for new samples. This is called transfer learning.</p>
                
                <p>Transfer learning is widely used in image classification and natural language processing. Image classifiers are often fine-tuned models, initially trained on large datasets with millions of pictures like ImageNet. Language models will be pre-trained on large corpuses of text with billions of words. This leads to more efficient training and less overfitting on smaller, targeted datasets. The same can be done for protein sequences, using transfer learning from a larger project to aid your smaller dataset.</p>
                
                <h2 id="solution">Solution</h2>
                
                <p>One solution to the protein sequence embedding problem comes in the form of a paper from Bepler and Berger released in 2018 and published at the 2019 ICLR. (https://openreview.net/forum?id=SygLehCqtm). The authors produced a set of recurrent neural networks trained on generic protein prediction tasks, like secondary structure prediction, contact map prediction and transmembrane prediction. The intermediate representations of those pre-trained networks can be used to annotate your dataset and enhance your predictor’s performance.</p>
                
                <h3>Instructions</h3>
                <p>This tutorial assumes some prior knowledge of Python programming and machine learning. This is not a beginner’s tutorial. There are excellent resources available online to teach you this stuff, for example on https://reddit.com/r/learnmachinelearning .</p>
                
                <h4>1: Clone GitHub Repo &amp; Download models</h4>
                
                <p>First, use Git to clone the repository from the authors, it is located here: https://github.com/tbepler/protein-sequence-embedding-iclr2019 . While you’re there, leave a star if you want to show your appreciation! ★</p>

                <p>Second, download the pretrained models, the link is listed in the repository README. Untar this package to find the PyTorch models saved in binary form.</p>

                <h4>2: Baseline example for comparison</h4>
                <p>The FireProt Protein Thermostability database will be used in order to demonstrate the use of these sequence embeddings. Thermostability predictions are a rather difficult affair, but maybe some sequence embeddings can help.</p>
                
                <p>Start by loading the data from the CSV into a pandas dataframe and retrieving the corresponding sequences. Because the PDB identifiers are given, we can use the RCSB website to retrieve the relevant sequences.</p>
                
                <p>The sequences are, of course, in their wildtype form. This is valuable, but every proposed variant will yield different results. Thus, we also want to generate an embedding for the mutated protein sequence. </p>
                
                <p>For this example, I will evaluate the performance based on the Pearson correlation, with a very simple model with minimal features as a base: The base model is a RandomForestRegressor with 200 estimators and 3 features: Secondary structure, wildtype and varianttype. In 10 fold cross validation, this classifier performs as a baseline, with a 0.18 PearsonR score. </p>
                
                <p>A special case of 10 fold cross validation is used, where the training sets contain only proteins that are not present in the test sets. This helps in ensuring that thermostability features are learning, instead of the distribution of the protein. Note that this is a difficult problem, especially with a small dataset! It is also the best way to avoid fooling oneself when evaluating the improvements gained from adding these features.</p>
                
                <h4>3: Adding vectors</h4>
                <p>Using the pretrained models we can generate vectors that describe each sequence. The models in question are created in PyTorch, which means we need to prepare the input data in a PyTorch format and move the results back to numpy. On the upside, we are able to the GPU to speed up these operations. An example sequence-to-embedding conversion is listed here:</p>
                
                <h4>4: Combining the vectors</h4>
                <p>In the spirit of performing a sort-of scientific analysis as part of this article, I have outlined some factors to be tested and generated a number of competing hypotheses for these factors.</p>
                
                <p>The first axis of testing will be the model used for embedding generation. 4 distinct models with the same embedding structure have been trained on different tasks. I will test all of them, each with the full embedding component including the projection to 100 features, as well as only the first component with 512 features. The former may yield a more concise, relevant featureset, whereas the latter is able to represent the structure in more detail.</p>
                
                <p>The second axis is how to reduce the embedding matrix to a vector. Three different approaches can be used to achieve this:</p>
                
                <ol>
                    <li>Taking the mean of the entire matrix. This can represent the structure in its entirety.</li>
                    <li>Taking the final vector of the matrix. The network structure uses a LSTM module, which accumulates state over the sequential axis. The final vector thus would represent the full structure in itself.</li>
                    <li>Taking the vector at the position of the variant. The LSTM module in this structure is bidirectional. The vector at the position of the variant would thus include data from both before and after that position. The data would also be vary more between positions and the features would be local to the position.</li>
                </ol>
                
                <p>Finally, it can be useful to distinguish between the activations of the sequence with the variant introduced and the wildtype features. To that end, three different combinations of these vectors can be produced:</p>
                
                <ol>
                    <li>Only the vector based on the sequence with the variant. This is the cleanest representation.</li>
                    <li>Only the vector based on the wildtype variant. This provides a lot of consistent data for the algorithm to learn from.</li>
                    <li>The vector based on the sequence with the variant and the wildtype concatenated. This option yields the most information.</li>
                    <li>The variant and wildtype sequence vector subtracted from one another. This final option may capture the relevance of any differences between the sequence representations.</li>
                </ol>
                
                <p>All these 10 fold cross validation experiments were executed 3 times with different seeds, the results are the mean of these 3 experiments. </p>
                
                ----
                
                <p>In order to find the effects of these different experiments in relation to one another, it is useful to generate a correlation matrix. Different models may yield similar performances, but vary in their actual output. Disparate models may be integrated in an ensemble model for increased performance (the combination of two different models is better than either one on its own). This is left as an exercise to the reader.</p>
                
                <h4>5: Convert to training set</h4>
                <p>Once we have converted our torch tensors back to numpy tensors on the CPU, we can add these to our training dataframe. We will need to generate some column names.</p>
                
                <h4>6: Building a better classifier</h4>
                <p>Based on the experiments, we can build a basic classifier to perform classification and</p>
                
                <h2>Conclusion</h2>
                <p>In this article, we looked at why it is useful to add protein sequence data to bioinformatics machine learning feature sets, what it means to include this data and how it is done in practice. If you followed along, you should have a thermostability regressor that  can adequately predict the ΔΔG for a given variant. You should also have some insights in the best practices of using these vectors, as well as a jumping off point for the creation of your own models.</p>
                
                <h2>References</h2>
                <ol>
                    <li>
                        The smallest protein. 
                        <a href="https://www.science20.com/princerain/blog/smallest_protein">link <i class="fa fa-link"></i></a>
                    </li>
                    <li>
                        The complete gene sequence of titin, expression of an unusual approximately 700-kDa titin isoform, and its interaction with obscurin identify a novel Z-line to I-band linking system. Bang et al. 2001 
                        <a href="https://www.ncbi.nlm.nih.gov/pubmed/11717165">link <i class="fa fa-link"></i></a>
                    </li>
                    <li>
                        How big is the average protein?
                        <a href="http://book.bionumbers.org/how-big-is-the-average-protein/">link <i class="fa fa-link"></i></a>
                    </li>
                    <li>
                        XKCD: Machine Learning
                        <a href="https://xkcd.com/1838/">link <i class="fa fa-link"></i></a>
                    </li>
                    <li>
                        Neural Probabilistic Language Models. Bengio et al. 2006
                        <a href="https://link.springer.com/chapter/10.1007%2F3-540-33486-6_6">link <i class="fa fa-link"></i></a>
                    </li>
                    <li>
                        Distributed Representations of Words and Phrases and their Compositionality.
                        Mikolov et al. 2013
                        <a href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality">link <i class="fa fa-link"></i></a>
                    </li>
                    <li>
                        Learning protein sequence embeddings using information from structure. Beplet et al. 2018
                        <a href="https://openreview.net/forum?id=SygLehCqtm">link <i class="fa fa-link"></i></a>
                    </li>
                
                </ol>
                
                <p>&nbsp;</p>
                
            </div>        
        </div>       
    </body>
</html>
