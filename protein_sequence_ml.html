<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Using Protein Sequences in Machine Learning - Stephan Heijl</title>
    <meta name="description" content="Tutorial on using protein sequence embeddings as features for machine learning algorithms.">
    <meta name="twitter:card" content="summary" />
    <meta name="twitter:title" content="Using protein sequences to make better classifiers" />
    <meta name="twitter:description" content="In this intermediate-level tutorial we use protein sequence embeddings as features for machine learning." />
    <meta name="twitter:image" content="https://stephanheijl.com/img/membrane_protein.png" />
    <link href="https://fonts.googleapis.com/css2?family=Newsreader:ital,opsz,wght@0,6..72,400;0,6..72,500;1,6..72,400&family=Outfit:wght@300;400;500;600&family=JetBrains+Mono:wght@400&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="style-v2.css">
    <link rel="stylesheet" href="/prism.css" />
    <style>
        .article-content pre {
            background: var(--bg-dark);
            color: #f6f8ff;
            padding: 1.5rem;
            overflow-x: auto;
            margin: 2rem 0;
            font-family: 'JetBrains Mono', monospace;
            font-size: 0.85rem;
            line-height: 1.5;
        }
        .article-content code {
            font-family: 'JetBrains Mono', monospace;
        }
    </style>
</head>
<body>
    <!-- Progress Bar -->
    <div class="progress-bar" id="progressBar"></div>

    <!-- Navigation -->
    <nav>
        <a href="index.html" class="logo">Stephan Heijl</a>
        <ul class="nav-links">
            <li><a href="projects.html">Work</a></li>
            <li><a href="projects.html#posts">Writing</a></li>
            <li><a href="projects.html#education">Background</a></li>
            <li><a href="contact.html">Contact</a></li>
        </ul>
    </nav>

    <!-- Article Header -->
    <header class="article-header">
        <div class="breadcrumb">
            <a href="index.html">Home</a>
            <span>-></span>
            <a href="projects.html#posts">Writing</a>
            <span>-></span>
            <span>Protein Sequence ML</span>
        </div>

        <div class="article-meta">
            <span class="meta-tag">Tutorial</span>
            <span class="meta-item">Posted: August 15, 2019</span>
            <span class="meta-item">Updated: October 11, 2019</span>
        </div>

        <h1>Using Protein Sequences in Machine Learning</h1>

        <p class="article-intro">
            In this intermediate-level tutorial we use protein sequence embeddings as features for
            machine learning algorithms to improve their performance.
        </p>
    </header>

    <!-- Article Content -->
    <article class="article-content">
        <h2>Introduction</h2>

        <p>
            In bioinformatics, we often want to predict biological phenomena when interactions are complex and
            not directly correlated to what's being studied. Machine learning helps—but what if we could also
            use protein sequence data?
        </p>

        <h3>Use Cases</h3>

        <p><strong>Transmembrane prediction:</strong> Classify exact transmembrane protein types from sequence.</p>

        <figure class="article-figure">
            <img src="img/membrane_protein.png" alt="Membrane protein"/>
            <figcaption>A protein positioned in a membrane (Dr. Thomas Splettstoesser, CC-BY-SA)</figcaption>
        </figure>

        <p><strong>Protein similarity:</strong> Predict structural similarity despite sequence variation.</p>

        <p><strong>Thermostability:</strong> Predict which variants affect protein temperature tolerance.</p>

        <h3>The Problem</h3>

        <p>Most ML algorithms struggle with sequence data because:</p>

        <ul>
            <li>Sequences vary in length</li>
            <li>Relative position matters, not absolute</li>
            <li>Sequences are symbolic, not real-valued</li>
            <li>Cannot be resized like images</li>
            <li>Biological data is often sparse</li>
        </ul>

        <h2>Theory</h2>

        <h3>Embeddings</h3>

        <p>
            Neural networks in NLP produce <strong>distributed representations of words</strong>—real-valued
            vectors that describe meaning in N-dimensional space. The famous "king - man = queen" expression
            demonstrates this.
        </p>

        <figure class="article-figure">
            <img src="img/word_embeddings.png" alt="Word embeddings visualization"/>
            <figcaption>Words projected into latent space encoding meaning</figcaption>
        </figure>

        <h3>Transfer Learning</h3>

        <p>
            Biological datasets are often small. Transfer learning imbues models with information from outside
            your dataset. Image classifiers use ImageNet, language models use billion-word corpuses—the same
            applies to protein sequences.
        </p>

        <h2>Solution</h2>

        <p>
            <a href="https://openreview.net/forum?id=SygLehCqtm">Bepler and Berger (2018)</a> produced recurrent
            neural networks trained on generic protein prediction tasks. Their intermediate representations can
            enhance your predictor's performance.
        </p>

        <h3>Implementation Steps</h3>

        <ol>
            <li>Clone the <a href="https://github.com/tbepler/protein-sequence-embedding-iclr2019">GitHub repo</a></li>
            <li>Download pretrained models</li>
            <li>Create baseline predictor for comparison</li>
            <li>Generate sequence embeddings using pretrained models</li>
            <li>Combine embeddings with your features</li>
        </ol>

        <h3>Key Findings</h3>

        <p>From experimental testing on thermostability prediction:</p>

        <ul>
            <li>Taking the <strong>average</strong> of the entire sequence yielded best results</li>
            <li>Combining variant + wildtype vectors was most effective</li>
            <li>Subtracting vectors was <strong>not</strong> effective</li>
            <li>Language model instances outperformed projection-level models</li>
        </ul>

        <p>
            The best configurations scored <strong>0.10+ correlation points higher</strong> than baseline—a
            valuable improvement for difficult prediction tasks.
        </p>

        <h2>Conclusion</h2>

        <p>
            Protein sequence embeddings can significantly improve bioinformatics ML features. This method works
            for any prediction task with available protein sequences and requires no extra data gathering.
            Combined with hand-engineered features, even greater effects are possible.
        </p>

        <h2>References</h2>

        <ol>
            <li><a href="https://openreview.net/forum?id=SygLehCqtm">Learning protein sequence embeddings (Bepler & Berger, 2018)</a></li>
            <li><a href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality">Word2Vec (Mikolov et al., 2013)</a></li>
            <li><a href="https://link.springer.com/chapter/10.1007%2F3-540-33486-6_6">Neural Probabilistic Language Models (Bengio et al., 2006)</a></li>
        </ol>
    </article>

    <!-- Article Footer -->
    <div class="article-footer">
        <a href="projects.html#posts" class="back-link">
            <svg width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1.5">
                <path d="M19 12H5M12 19l-7-7 7-7"/>
            </svg>
            Back to Writing
        </a>
    </div>

    <!-- Footer -->
    <footer>
        <p class="footer-text">© 2024 Stephan Heijl</p>
        <div class="footer-links">
            <a href="https://github.com/StephanHeijl">GitHub</a>
            <a href="https://linkedin.com/in/stephanheijl">LinkedIn</a>
            <a href="contact.html">Email</a>
        </div>
    </footer>

    <script>
        window.addEventListener('scroll', () => {
            const docHeight = document.documentElement.scrollHeight - window.innerHeight;
            const progress = (window.scrollY / docHeight) * 100;
            document.getElementById('progressBar').style.width = progress + '%';
        });
    </script>
    <script src="/prism.js"></script>
</body>
</html>
