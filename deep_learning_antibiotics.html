<html>
    <head>
        <title>StephanHeijl.com - Deep Learning Antibiotics</title>
        <meta name="description" content="A blog post detailing the recent innovation by Stokes et al where deep neural networks were used to find a novel antibiotic molecule.">
        <meta name="keywords" content="antiobiotics,deep learning,chemprop,explained,halicin">
        <meta name="robots" content="index, follow">
        <meta name="language" content="EN">
        <meta name="author" content="Stephan Heijl">
        <meta name="creationdate" content="17/3/2020">
        <meta name="distribution" content="global">
        <meta name="rating" content="general">
        <link rel="stylesheet" href="style.css" />
        <link rel="stylesheet" href="gradients.css" />
        <script src="zepto.min.js"></script>
        <script src="cards.js"></script>
        <link href="https://fonts.googleapis.com/css?family=Dosis|Open+Sans" rel="stylesheet">

        <script src="https://kit.fontawesome.com/af71bba04e.js" crossorigin="anonymous"></script>
        <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" />
        <style>
            table {
                border-spacing: 0;
            }
            td {
                padding: 5px;
            }
            thead {
                background: rgb(214, 230, 255);
                font-weight: bold;
                
            }
            tr:nth-child(even) {
                background: rgb(227, 236, 250);;
            }
            
            ol > ol {
                list-style-type: lower-roman;
            }
            
            ol > ol > ol {
                list-style-type: lower-alpha;
            }
        </style>
    </head>
    <body>
        <nav>
            <div class="internal-wrapper">
                <a class="logo" href="https://stephanheijl.com">StephanHeijl.com</a>
                <ul>
                    <li><a href="projects.html#projects">Projects</a></li>
                    <!--<li><a href="projects.html#publications">Publications</a></li>-->
                    <li><a href="projects.html#posts">Blog posts</a></li>
                    <li><a href="projects.html#education">Education</a></li>
                    <li><a href="contact.html">Contact</a></li>
                </ul>            
            </div>                     
        </nav>
        <header>
            <div class="internal-wrapper">
                <div class="header-card">
                    <div class="card-wrapper">
                        <div class="card gradient-paper">
                            <div class="glare"></div>
                            <div class="post-name">
                                Deep Learning Antibiotics <br>
                                <i class="fa fa-pills"></i>
                            </div>
                        </div>
                    </div>
                </div>
                <div class="header-annotation">
                    <h1>Deep Learning Antibiotics</h1>
                     <p>On february 20th 2020, Stokes et al published “A Deep Learning Approach to Antibiotic Discovery” in Cell. In this paper, the authors lay out their effort in using a deep learning model to predict antibiotic activity based on molecular structure. This model predicts halicin as an antibacterial molecule. The authors perform extensive testing to show that halicin functions as a broad-spectrum bactericidal antibiotic. This post will dive into some of the technical aspects behind this discovery, with specific attention to the deep learning model used.
                    </p>
                    <ul class="header-tags">
                        <li>Education</li>
                    </ul>
                </div>
            </div>
        </header>
        
       <div class="details">
    <div class="internal-wrapper">
      <iframe src='/chemprop_animations/message_passing_animation.html' style="border:0; height:200px; width: 600px; margin: 0 auto; display: block; margin-top: 2em;"></iframe>

      <ol>
        <li><a href="#introduction">Introduction</a></li>

        <li><a href="#gnn">Graph Neural Networks</a>
          
          <ol>
              <li><a href="#molecules-as-graphs">Molecules as graphs</a></li>
              <li><a href="#mpnn">Message Passing Neural Networks</a></li>
                  <ol>
                      <li><a href="#mpnn-mpp">Message Passing Phase</a></li>
                      <li><a href="#mpnn-readout">Readout Phase</a></li>
                  </ol>
              
            </ol>
            
        </li>
          
        <li><a href="#chemprop">chemprop</a>
          
          <ol>
              <li><a href="#atombondfeatures">Atom and bond features</a></li>
              <li><a href="#hyperopt">Hyperparameter optimization</a></li>
          </ol>
    
        </li>
        <li><a href="#stokes">Stokes et al. Methodology</a>
          <ol>
              <li><a href="#stokes-chemprop">chemprop use</a></li>

              <ol>

                  <li><a href="#architecture">Architecture</a></li>

                  <li><a href="#dataset">Dataset</a>a</li>

            <li><a href="human-itl">Active Learning</a></li>
                </ol>


            <li><a href="#mechanism">Halicin Mechanism of action</a></li>
          </ol>
        </li>
          <li><a href="#discussion">Discussion</a></li>
          <li><a href="#conclusion">Conclusion</a></li>
      </ol>
          <img src="img/chemprop/MessagePassingConvNet.png"/>
      <h2 id="introduction">Introduction</h2>

      <p>The space of molecules wherein antibiotics may occur is large and becoming
      larger. Many antibiotic discovery programs choose to look inside large existing
      chemical libraries to find new leads. These chemical libraries are limited in
      several ways, as they do not sufficiently cover the chemical space and are
      prohibitively expensive to curate and expand. According to Stokes et al:
      &ldquo;Since the implementation of high-throughput screening in the 1980s, no new
      clinical antibiotics have been discovered using this method.&rdquo; Most
      &lsquo;new&rsquo; finds turn out to be structurally very similar to older
      antibiotics, which reduces their potential for effectiveness.</p>

      <p>The authors note that machine learning has advanced to the point where it may
      present an alternative to this approach of finding antibiotics. In 2016,
      researchers already reported generating representations of molecules from a
      chemical compound space using deep learning (Gomez-Bombarelli et al). The
      innovation of deep learning primarily lies in its ability to describe the chemicals
      itself, instead of relying on manually designed annotations. This should aid in
      their ability to predict chemical properties of molecules.</p>

      <p>In short, the prediction procedure was as follows:</p>

      <ol>
        <li>A deep graph-based neural network was trained to predict growth inhibition of
        E. coli with a dataset of 2335 molecules. These were derived from an FDA approved
        drug library and library of natural products isolated from plant, animal and
        microbial sources.</li>

        <li>This model was used to predict the E. coli growth inhibiting properties on
        the Drug Repurposing Hub library, containing 6111 different molecules. The top 99
        and bottom 63 molecules were empirically tested. The model scores 51% true
        positive rate and 97% true negative rate. Halicin was one of the true positives,
        and given its other properties was deemed viable for further testing.</li>

        <li>The results of the previous test were inserted into the library and the model
        was retrained.</li>

        <li>The model was then used to predict E. coli growth inhibiting properties on a
        subset of the ZINC15 chemical library containing over 170 million molecules.</li>
      </ol>

      <p>The neural network used is a message passing network called chemprop. It is
      publically available on GitHub: <a href=
      "https://github.com/chemprop/chemprop">
      https://github.com/chemprop/chemprop</a> and development has been ongoing since at
      least June 12th 2018. Shared second authors Kevin Yang and Kyle Swanson (order as
      shown in the author listing for the paper) published this network in JCIM in 2019
      (Yang et al). The fork network on GitHub shows that several others have already
      been working with this neural network for other purposes, like property prediction
      of 2-molecule mixtures by Allison Tam at MIT. We can expect more results based on
      this model later this month (<a href=
      "https://www.aidm.mit.edu/posters1">https://www.aidm.mit.edu/posters1</a>).</p>

      <h2 id="gnn">Graph Neural Networks</h2>

      <h3 id="molecules-as-graphs">Molecules as Graphs</h3>

      <p>Molecules are groups of atoms held together by chemical bonds. In chemistry
      molecules are often illustrated by connecting the letters representing the atoms
      with lines. Take for example, Halicin (less popularly known as:
      5-[(5-Nitro-1,3-thiazol-2-yl)sulfanyl]-1,3,4-thiadiazol-2-amine).</p>

      <p><img src="/img/chemprop/halicin.png" alt="Halicin structure"/></p>

      <p>Halicin&rsquo;s structure prominently features two rings. These rings consist of
      Sulfur (S) atoms and one or two Nitrogen (N) atoms. In addition, by convention
      Carbon (C) atoms are omitted in these drawings. They are placed where lines meet
      and no letters are present.<br />
      <br />
      Because all the atoms in a molecule are by definition (directly or indirectly)
      contiguous, it is easy to see how they could be converted into a graph. Atoms
      become nodes and the chemical bonds between them become vertices.</p>

      <p><img src="/img/chemprop/halicin_network_nodes.png" alt="Halicin structure as a graph"/></p>

      <p>In a graph we can assign properties to nodes, in this case we can annotate each
      note by which atom is present at each location.</p>

      <p><img src="/img/chemprop/halicin_network_nodes_color.png" alt="Halicin structure as a graph with color" /></p>

      <p>Some atoms are connected by double (or in the case of other molecules, even
      triple bonds). The vertices can be annotated to reflect this so that the network is
      aware of this.</p>

      <h3><img src="/img/chemprop/halicin_network_nodes_color_edges.png"  alt="Halicin structure with colored nodes and edges"/></h3>

      <p>The illustration here is still simplified, as hydrogen atoms are generally also
      omitted or combined with other atoms as groups in this visualization. Adding these
      would yield a more complex graph but equally valid graph.</p>

      <h3 id="mpnn">Message Passing Neural Networks</h3>

      <p>Message Passing Neural Networks (MPNNs) take advantage of the ability of
      molecules to be represented as graphs in order to compute compute meaningful
      representations of them. Molecules vary in size and in the number of nodes and
      edges they have, which makes it difficult for neural networks to perform operations
      with them. Using MPNNs we can compute a fixed size representation, which is easier
      for a neural network to ingest. MPNNs are also differentiable, which means that
      they can be incorporated into the network architecture. This allows the MPNN to be
      trained and finetuned to produce representations for a specific purpose.</p>

      <p>The MPNN computes its representation in two phases: A message passing phase and
      a readout phase. The former allows the network to iteratively build up properties
      of the molecule over set amount steps. The latter combines these properties into a
      fixed size vector that represents the molecule as a whole.</p>

      <h4 id="mpnn-mpp">Message Passing Phase</h4>

      <p>During the Message Passing Phase, each node and vertex in the graph has a hidden
      state associated with it. This hidden state includes properties like the type of
      atom or bond the node or vertex represents. These hidden states are updated during
      t timesteps. For every timestep the following operations occur:</p>
        
      <figure>
        <iframe src='/chemprop_animations/message_passing_animation.html' style="border:0; height:200px; width: 600px; margin: 0 auto; display: block; margin-top: 2em;"></iframe>
        <figcaption>
            <b>Figure 1:</b> Animation showing the message passing phase on a projection of Halicin.
        </figcaption>
      </figure>

      

      <ol>
        <li>First, for every node the message function is computed. This function takes
        in the hidden state of said node, the hidden state of a neighbour node and the
        hidden state of the vertex between them. The function must be differentiable. The
        output is called the message.</li>

        <li>This is repeated for every neighbour node of the source node. The messages
        are all summed to produce a single message vector.</li>

        <li>This message vector is run through an update function. This is again a
        differentiable function. It should output a vector that is the same size as the
        hidden state for the node. In the original paper this update function is a
        recurrent neural network.</li>

        <li>When new vectors have been computed for every node, the hidden states for the
        nodes are set to those vectors. The message and update functions can even be
        computed in parallel.</li>

        <li>This entire process is then repeated with the new hidden states for t
        iterations.</li>
      </ol>

      <p>The same message passing and update functions are used for every node and during
      every time step. This allows for faster computation and the ability to use larger
      hidden states. The message and update functions are usually some kind of neural
      network.</p>

      <p>The hidden states are only updated after computing messages for all the nodes.
      This allows this phase to be invariant to the order in which the nodes are
      processed and robust in the face of parallel execution.</p>

      <h4 id="mpnn-readout">Readout Phase</h4>

      <p>The readout phase of the MPNN takes place after the full message passing phase
      completes. The hidden states of all the nodes are passed into a readout function.
      If the readout function is invariant to the order of nodes, for example a simple
      sum operation, the full MPNN will also be invariant to the order of nodes. The
      result of the readout phase is the representation of the molecule. This
      representation can either be used as a fingerprint, or it can be a distribution or
      a scalar value that can be optimized for some target. The entire MPNN can be
      trained end-to-end on the property of a chemical. The representation can also be
      followed by a feed forward neural network.</p>

      <h3 id="chemprop">chemprop</h3>

      <p>chemprop is a Message Passing Neural Network, it builds upon the structure of
      the MPNN from 2017, but introduces several changes that increase its
      performance.</p>

      <ul>
        <li>chemprop calculates the messages over each vector instead of over each
        node.</li>

        <li>In addition to annotations of type for each atom and chemical bond,
        chemprop&rsquo;s best models include molecule level annotations from RDKit. These
        features help describe the molecule by imparting expert knowledge onto the
        graph.</li>

        <li>chemprop uses summation as an update function instead of a Gated Recurrent
        Unit.</li>

        <li>chemprop adds Dropout after each message passing step, presumably to improve
        generalization.</li>
      </ul>

      <h4 id="atombondfeatures">Atom and bond features</h4>

      <p>The following features are used to annotate each atom:</p>

      <table>
        <thead>
          <tr>
            <td>
              <p>Feature</p>
            </td>

            <td>
              <p>Description</p>
            </td>

            <td>
              <p>Dimensionality</p>
            </td>
          </tr>
        </thead>  
        <tbody>
          <tr>
            <td>
              <p>Type of atom</p>
            </td>

            <td>
              <p>The type of atom, for example C, O or N, ordered by atomic number</p>
            </td>

            <td>
              <p>One hot encoded, 100d</p>
            </td>
          </tr>

          <tr>
            <td>
              <p>Number of bonds</p>
            </td>

            <td>
              <p>Number of bonds the atom is involved in, represented as a 6 bit (0-63)
              integer.</p>
            </td>

            <td>
              <p>Integer, 6d</p>
            </td>
          </tr>

          <tr>
            <td>
              <p>Formal charge</p>
            </td>

            <td>
              <p>The electronic charge assigned to an atom, represented as a 5 bit (0-31)
              integer.</p>
            </td>

            <td>
              <p>Integer, 5d</p>
            </td>
          </tr>

          <tr>
            <td>
              <p>Chirality</p>
            </td>

            <td>
              <p>A molecule level feature indicating how this molecule can be
              superimposed on its mirror image.</p>
            </td>

            <td>
              <p>One hot encoded, 4d</p>
            </td>
          </tr>

          <tr>
            <td>
              <p>Number of bonded hydrogen atoms</p>
            </td>

            <td>
              <p>In addition to Hydrogen atoms being modelled as nodes, this feature adds
              the number of bonded hydrogen atoms to a molecule as an integer.</p>
            </td>

            <td>
              <p>Integer, 5d</p>
            </td>
          </tr>

          <tr>
            <td>
              <p>Hybridization</p>
            </td>

            <td>
              <p>Hybridization state, this tells the network about the number of
              electrons surrounding the atom.</p>
            </td>

            <td>
              <p>One hot encoded, 5d</p>
            </td>
          </tr>

          <tr>
            <td>
              <p>Aromaticity</p>
            </td>

            <td>
              <p>Whether or not the atom is a part of an aromatic ring.</p>
            </td>

            <td>
              <p>Boolean, 1d</p>
            </td>
          </tr>

          <tr>
            <td>
              <p>Atomic mass</p>
            </td>

            <td>
              <p>Mass of the atom divided by 100. This ranges from 0.01008 to
              (theoretically) 2.94</p>
            </td>

            <td>
              <p>Float, 1d</p>
            </td>
          </tr>
        </tbody>
      </table>

      <p>Of note is that many features which could be given as a (normalized) number are
      represented as one-hot encoded integers. This may aid the network in interpretation
      of these discrete numbers. Only the atomic mass is represented as a float. It is
      divided by 100 which means that a large part of the periodic table will have a
      value between 0 and 1, including a majority of the organic atoms (Figure 2). In
      addition, 200 molecular features are computed in silico by RDKit and added to this
      feature list. Features are normalized to mean 0 and standard deviation 1 before
      being fed into the network.</p>

      <figure>
        <img src="img/chemprop/periodic_table.png" /><br/>
        <figcaption>
            <b>Figure 2:</b> Periodic table of the elements with atomic masses below 1 labelled in blue. Original image by C. Miller, CC BY-SA.
        </figcaption>
      </figure>

      <p>The following features are added to each bond/vertex:</p>

      <table>
        <thead>
          <tr>
            <td>
              <p>Feature</p>
            </td>

            <td>
              <p>Description</p>
            </td>

            <td>
              <p>Dimensionality</p>
            </td>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>
              <p>Type of bond</p>
            </td>

            <td>
              <p>The type of bond, single, double, triple, or aromatic.</p>
            </td>

            <td>
              <p>One hot encoded, 4d</p>
            </td>
          </tr>

          <tr>
            <td>
              <p>Conjugated bond</p>
            </td>

            <td>
              <p>Whether or not the bond is conjugated</p>
            </td>

            <td>
              <p>Boolean, 1d</p>
            </td>
          </tr>

          <tr>
            <td>
              <p>In ring</p>
            </td>

            <td>
              <p>Whether or not the bond is in a ring.</p>
            </td>

            <td>
              <p>Boolean, 1d</p>
            </td>
          </tr>

          <tr>
            <td>
              <p>Stereo</p>
            </td>

            <td>
              <p>The type of stereoisomerism of this bond.</p>
            </td>

            <td>
              <p>One hot encoded, 6d</p>
            </td>
          </tr>
        </tbody>
      </table>

      <h4 id="hyperopt">Hyperparameter optimization</h4>

      <p>Out of the box chemprop supports hyperparameter optimization. The authors use
      Bayesian Optimization to tune a number of aspects of the model. The following
      hyperparameters are tunable by default.</p>

      <ul>
        <li>Model depth, or the number of timesteps used during message passing. A larger
        number of steps means that atom and bond states are propagated over a longer
        distance. Information from one side of the molecule takes time to traverse to the
        other side.</li>
          
          
    <figure>
        <iframe src='/chemprop_animations/message_propagation_animation.html' style="border:0; height:310px; width: 700px; margin: 0 auto; display: block;"></iframe>
        <figcaption>
            <b>Figure 3:</b> Animation of the effects of model depth. 9 steps are required to propagate information from one side of the atom graph to the other side.
        </figcaption>
      </figure>
          
          

        <li>Hidden size of the bonds vectors.</li>

        <li>Number of feed forward layers in the feed forward network that follows the
        MPNN.</li>

        <li>Dropout in the MPNN.</li>
      </ul>

      <h2 id="stokes">Stokes et al Methodology</h2>

      <h3 id="stokes-chemprop">chemprop Use</h3>

      <p>Stokes et al recognize the need for in silico methods of determining molecule
      properties. Being able to predict how a molecule is going to behave under specific
      circumstances without having to run an experiment speeds up research and lowers
      research costs. The authors train chemprop to predict the effect of molecules on
      the growth of E. coli, a model organism used in the study of bacteria. The neural
      network is initially trained in 2335 molecules with annotations.</p>

      <h4 id="architecture">Architecture</h4>

      <p>chemprop is basically used as is, with the following hyperparameters:</p>

      <ul>
        <li>Message passing steps: 6</li>

        <li>Neural Network hidden size: 2200</li>

        <li>Number of feed-forward layers: 3</li>

        <li>Dropout probability: 0.15</li>
      </ul>

      <p>These hyperparameters were found using the built-in hyperopt system.</p>

      <h4 id="dataset">Dataset</h4>

      <p>Stokes et al initially measured the growth inhibition effects of 2335 different
      molecules. The molecules were selected from different resources to ensure
      diversity. For the purposes of training the growth inhibition was binarized, with a
      cut off value of 80% inhibition. This led to 120 &lsquo;hits&rsquo; or molecules
      with a significant growth inhibiting effect.</p>

      <p>Of note with regards to this dataset are two salient points:</p>

      <ol>
        <li>The dataset is small. Image recognition algorithms are trained on hundreds of
        thousands of images. This dataset is orders of magnitudes smaller.</li>

        <li>The dataset is unbalanced. Only 5% of the samples constitute hits. An
        imbalance in the dataset makes it more difficult to train a deep learning
        network.</li>
      </ol>

      <p>Yang et al note in the chemprop paper that their model underperforms on severely
      imbalanced datasets (a dataset with 0.2% hits is given as an example). Either the
      authors found a way to mitigate this, or this particular dataset was balanced
      enough so as not to cause stability problems.</p>

      <p>An ensemble of 20 models was used to improve performance.</p>

      <h3 id="human-itl">Active Learning/Human in the loop learning</h3>

      <p>chemprop was trained on 2335 molecules in the first phase of this project. In
      the second phase the authors predicted the growth inhibiting properties of 6111
      molecules and used the results to build a new dataset. These molecules were derived
      from the Drug Repurposing Hub, which contains drug molecules at various stages of
      testing. The 99 top performing and 63 lowest performing molecules were selected
      from this group and assessed for E. coli growth inhibition. These molecules were
      experimentally tested for growth inhibition and the resulting data were fed back
      into the model. This constitutes a form of human in the loop learning, where a
      human sits in the training loop for a machine learning model. These steps could be
      repeated indefinitely with, presumably better performance on each iteration.
      Strictly speaking it is not a form of active learning, as the algorithm is not able
      to actively query a human itself.</p>

      <h3 id="mechanism">Halicin mechanism of action</h3>

      <p>The authors note that because halicin retained its growth inhibiting activity
      even in E. coli and M. tuberculosis colonies that were mutated to be antibiotic
      resistant, its mechanism of action must be unconventional. In an effort to discover
      the way in which the molecule inhibits growth the authors attempted to evolve E.
      coli mutants that were halicin resistant. They were unable to find a resistant
      mutant in two different trials.</p>

      <p>RNA sequencing was used as an alternative method to discover the mechanism of
      action. It was found that when bacteria were exposed to less-than-lethal
      concentrations of halicin genes that are related to motility (movement) were
      expressed less frequently. Genes that preserve the balance of iron concentration in
      the cell were expressed more frequently. This led the authors to believe that
      halicin affected the ability of the cell to maintain a difference in
      electrochemical charge between the inside and the outside of the cell. This
      gradient is integral to the ability of a bacterium to move around. Its disturbance
      can either be caused by a means of altering the difference in electric potential
      (&Delta;&psi;) or the difference in pH.</p>

      <p>In order to test this hypothesis, halicin effectivity was tested in higher pH
      environments. The electrochemical gradient is retained in part by a difference in
      pH between the inside and the outside of the cell. As pH increased, halicin became
      less effective. This points to halicins mechanism of action involving the ability
      of bacteria to maintain a difference in pH.</p>

      <img src="img/chemprop/halicin_ph.png" alt="The higher the pH, the more halicin is required to inhibit growth. // pH 5.4: 0.5ug/ml required for >90% growth inhibition. // pH 6.4: 1ug/ml required for >90% growth inhibition. // pH 7.4: 2ug/ml required for >90% growth inhibition. // pH 8.4: 20ug/ml required for >90% growth inhibition. // pH 9.4: 100+ ug/ml required for >90% growth inhibition."/>

      <p>A final test was performed where bacteria were exposed to a fluorescent molecule
      (DISC) that becomes less fluorescent in higher concentrations. When the electric
      potential difference is disturbed, DISC is released into the environment, resulting
      in higher fluorescence. When the difference in pH is disturbed, the uptake of DISC
      is increased which results in lower fluorescence.</p>

      <img src="img/chemprop/halicin_fluorescence.png" alt="Higher halicin concentrations are correlated with lower DISC fluoresence, indidating disturbance of pH gradient. // PMB (disturbs electric potential) -> increased fluorescence. // DMSO (neutral substance) -> no change in fluorescence. // Halicin 2 ug/ml -> 1400 RFU decrease in fluorescence. // Halicin 4 ug/ml -> 1800 RFU decrease in fluorescence. // Halicin 8 ug/ml -> 4400 RFU decrease in fluorescence." />

      <h2 id="discussion">Discussion</h2>

      <p>In this section I humbly submit some thoughts on the chemprop network. Note that
      I have not worked on or with the network, this is just &ldquo;thinking aloud&rdquo;
      as it were. I invite anyone to comment on these and correct me where I&rsquo;m
      wrong.</p>

      <h3>Dataset imbalance</h3>

      <p>The dataset used by Stokes et al is small and unbalanced. Using techniques like
      class weighting or bootstrapping can help in rebalancing the dataset, which could
      lead to better performance.</p>

      <h3>Pretraining chemprop</h3>

      <p>In Natural Language Processing we find that increases in the state of the art
      performance have come from pre-training large neural nets in self-supervised
      settings. That is: models are given part of an input and expected to fill in the
      blanks. This requires no labeling and allows the model to get the structure of the
      training language imparted upon it before being exposed to a task-specific dataset.
      chemprop might benefit from a pre-training step, where it could predict parts of a
      molecule that are masked out from the graph. The number of molecules for which a
      structure is available is vast, requiring no labeling. This might aid the model in
      creating an internal representation of the existing molecule space, which could
      speed up training and/or improve performance and generalizability.</p>

      <h3>Attention mechanisms</h3>

      <p>Attention is a concept in deep learning that has also become more prevalent
      since the introduction of Transformers to the Natural Language Processing field.
      Simply put, attention mechanisms allow a neural network to selectively pay
      attention to parts or regions of the input. Given the benefits it has had for NLP,
      introducing attention to chemprop may also yield increased performance.</p>

      <figure>
        <img src="img/chemprop/halicin_attention_network.png" />
        <figcaption>
            <b>Figure 4:</b> Illustration of a proposed attention network base on  chemprop.
        </figcaption>
        </figure>

      <h2 id="conclusion">Conclusion</h2>

      <p>Jonathan Stokes, Keving Yang, Kyle Swanson and the other authors of the &ldquo;A
      Deep Learning Approach to Antibiotic Discovery&rdquo; and &ldquo;Analyzing Learned
      Molecular Representations for Property Prediction&rdquo; papers made an important
      contribution to the scientific intersection of machine learning and biology.
      Collaboration that applies innovations in deep learning to biology and healthcare
      will be an important source of new discoveries in the field and improvements of
      life in the coming decade.</p>

      <p>It remains to be seen if halicin or any of the other compounds that were
      designated as leads by the model will pass the required clinical trials, but given
      that the molecules are derived from a source of existing drugs their chances are
      substantial. I certainly hope that these new drugs can advance quickly and aid
      humanity in its fight against antibiotic resistant bacteria.</p>

      <h2>References:</h2>

      <p>Stokes et al., Cell, February 20, 2020 <br/><a href=
      "https://www.cell.com/cell/fulltext/S0092-8674(20)30102-1">
      https://www.cell.com/cell/fulltext/S0092-8674(20)30102-1</a>
        </p>
      <p>
      Gomez Bombarelli et al.<br/> ACS Cent. Sci. 2018, 4, 2, 268-276 <br/>
Publication Date:January 12, 2018<br/>
<a href=
      "https://doi.org/10.1021/acscentsci.7b00572">
    https://doi.org/10.1021/acscentsci.7b00572</a></p>
      <p>
        Yang et al. <br/>
          J. Chem. Inf. Model. 2019, 59, 8, 3370-3388<br/>
Publication Date:July 30, 2019<br/>
<a href="https://doi.org/10.1021/acs.jcim.9b00237">https://doi.org/10.1021/acs.jcim.9b00237</a>
          </p>        
    <p>
      Implementation of MPNN:<br/> <a href=
      "https://github.com/deepchem/deepchem/blob/master/deepchem/models/tensorgraph/graph_layers.py%23L384">
      https://github.com/deepchem/deepchem/blob/master/deepchem/models/tensorgraph/graph_layers.py#L384</a></p>

      <p>&nbsp;</p>
    </div>
  </div> 
        <script async defer src="https://cdn.simpleanalytics.io/hello.js"></script>
<noscript><img src="https://api.simpleanalytics.io/hello.gif" alt=""></noscript>
    </body>
</html>
